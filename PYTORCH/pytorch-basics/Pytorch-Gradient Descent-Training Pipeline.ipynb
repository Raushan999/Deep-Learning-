{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea9fb7f",
   "metadata": {},
   "source": [
    "#  Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817db370",
   "metadata": {},
   "source": [
    "Using Numpy: Doing all manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7dd5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first manually calculate the gradient for linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc17826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31146315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization: liear regression  [ y = 2*x ]\n",
    "x = np.array([1,2,3,4],dtype = np.float32)\n",
    "y = np.array([2,4,6,8],dtype = np.float32)\n",
    "# weight\n",
    "w = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2225f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#Loss  = MSE (for linear regression)\n",
    "def loss(y,y_pred):\n",
    "    return ((y_pred-y)**2).mean()\n",
    "\n",
    "# gradient of the loss function\n",
    "# MSE = 1/N(w*x - y)**2\n",
    "# d(loss)/dw = 1/N.(2x).(w*x-y)\n",
    "def gradient(x,y,y_pred):\n",
    "    return np.dot(2*x, (y_pred-y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8940411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch: 1: weight = 1.200,loss=30.00000000\n",
      "epoch: 2: weight = 1.680,loss=4.79999924\n",
      "epoch: 3: weight = 1.872,loss=0.76800019\n",
      "epoch: 4: weight = 1.949,loss=0.12288000\n",
      "epoch: 5: weight = 1.980,loss=0.01966083\n",
      "epoch: 6: weight = 1.992,loss=0.00314574\n",
      "epoch: 7: weight = 1.997,loss=0.00050331\n",
      "epoch: 8: weight = 1.999,loss=0.00008053\n",
      "epoch: 9: weight = 1.999,loss=0.00001288\n",
      "epoch: 10: weight = 2.000,loss=0.00000206\n",
      "Prediction after training:f(5) = 9.99895\n"
     ]
    }
   ],
   "source": [
    "# f(5) = w*x = 0*5 ---> 0\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "# Prediction\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = forward(x)\n",
    "    # loss\n",
    "    l = loss(y,y_pred)\n",
    "    # gradients\n",
    "    dw = gradient(x,y,y_pred)\n",
    "    # update the gradient.\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        print(f'epoch: {epoch+1}: weight = {w:.3f},loss={l:.8f}')\n",
    "        \n",
    "\n",
    "# Final prediction\n",
    "print(f'Prediction after training:f(5) = {forward(5):0.5f}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51ed19",
   "metadata": {},
   "source": [
    "We see that the weights are getting updataed and loss is decreasing.\n",
    "Final output ideally should be 10, but it is almost around that\n",
    "And for higher iterations it will become more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ca6dc",
   "metadata": {},
   "source": [
    "# Using Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ad6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c87885bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization: liear regression  [ y = 2*x ]\n",
    "x = torch.tensor([1,2,3,4],dtype = torch.float32)\n",
    "y = torch.tensor([2,4,6,8],dtype = torch.float32)\n",
    "# weight\n",
    "w = torch.tensor(0.0, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ce7378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#Loss  = MSE (for linear regression)\n",
    "def loss(y,y_pred):\n",
    "    return ((y_pred-y)**2).mean()\n",
    "\n",
    "# gradient of the loss function\n",
    "# MSE = 1/N(w*x - y)**2\n",
    "# d(loss)/dw = 1/N.(2x).(w*x-y)\n",
    "def gradient(x,y,y_pred):\n",
    "    return torch.dot(2*x, (y_pred-y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0246377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch: 1: weight = 0.300,loss=30.00000000\n",
      "epoch: 3: weight = 0.772,loss=15.66018772\n",
      "epoch: 5: weight = 1.113,loss=8.17471695\n",
      "epoch: 7: weight = 1.359,loss=4.26725292\n",
      "epoch: 9: weight = 1.537,loss=2.22753215\n",
      "epoch: 11: weight = 1.665,loss=1.16278565\n",
      "epoch: 13: weight = 1.758,loss=0.60698116\n",
      "epoch: 15: weight = 1.825,loss=0.31684780\n",
      "epoch: 17: weight = 1.874,loss=0.16539653\n",
      "epoch: 19: weight = 1.909,loss=0.08633806\n",
      "Prediction after training:f(5) = 9.61241\n"
     ]
    }
   ],
   "source": [
    "# f(5) = w*x = 0*5 ---> 0\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "# Prediction\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(y,y_pred)\n",
    "    \n",
    "    # backward prop: dl/dw\n",
    "    l.backward()\n",
    "    # update  the gradient.\n",
    "    with torch.no_grad(): # w.grad should be free from prev iter.\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero gradient: w.grad = 0\n",
    "    w.grad.zero_() \n",
    "    if epoch%2==0:\n",
    "        print(f'epoch: {epoch+1}: weight = {w:.3f},loss={l:.8f}')\n",
    "        \n",
    "\n",
    "# Final prediction\n",
    "print(f'Prediction after training:f(5) = {forward(5):0.5f}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91143c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bf30b1b",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e76e69",
   "metadata": {},
   "source": [
    "Steps we need to follow:\n",
    "1. Design model(input,output_size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training Loop\n",
    "   - forward Pass: compute prediction\n",
    "   - backward pass: gradient\n",
    "   - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "457dd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12bb287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization: liear regression  [ y = 2*x ]\n",
    "x = torch.tensor([1,2,3,4],dtype = torch.float32)\n",
    "y = torch.tensor([2,4,6,8],dtype = torch.float32)\n",
    "# weight\n",
    "w = torch.tensor(0.0, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3479eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#Loss callback  = MSE (for linear regression)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# calling optimizer: SGD\n",
    "optimizer = torch.optim.SGD([w],lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72825747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch: 1: weight = 0.300,loss=30.00000000\n",
      "epoch: 3: weight = 0.772,loss=15.66018772\n",
      "epoch: 5: weight = 1.113,loss=8.17471695\n",
      "epoch: 7: weight = 1.359,loss=4.26725292\n",
      "epoch: 9: weight = 1.537,loss=2.22753215\n",
      "epoch: 11: weight = 1.665,loss=1.16278565\n",
      "epoch: 13: weight = 1.758,loss=0.60698116\n",
      "epoch: 15: weight = 1.825,loss=0.31684780\n",
      "epoch: 17: weight = 1.874,loss=0.16539653\n",
      "epoch: 19: weight = 1.909,loss=0.08633806\n",
      "Prediction after training:f(5) = 9.61241\n"
     ]
    }
   ],
   "source": [
    "# f(5) = w*x = 0*5 ---> 0\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "\n",
    "# Prediction\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(y,y_pred)\n",
    "    \n",
    "    # backward prop: dl/dw\n",
    "    l.backward()\n",
    "    # update weights:\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # zero gradient: w.grad = 0\n",
    "    optimizer.zero_grad() \n",
    "    if epoch%2==0:\n",
    "        print(f'epoch: {epoch+1}: weight = {w:.3f},loss={l:.8f}')\n",
    "        \n",
    "\n",
    "# Final prediction\n",
    "print(f'Prediction after training:f(5) = {forward(5):0.5f}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e25d71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's replace manually implemented forward method with PyTorch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b016e7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n"
     ]
    }
   ],
   "source": [
    "# Initialization: liear regression : 2-d tensor now\n",
    "x = torch.tensor([[1],[2],[3],[4]],dtype = torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]],dtype = torch.float32)\n",
    "\n",
    "# size of input, features\n",
    "n_samples, n_features = x.shape # 4*1\n",
    "print(n_samples,n_features)\n",
    "input_size = n_features\n",
    "output_size = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight: we don't really need to define weights.\n",
    "# Model is Linear regression model\n",
    "model = nn.Linear(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c798d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.989\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.tensor([5.],dtype = torch.float32)\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "381bafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "\n",
    "#Loss callback  = MSE (for linear regression)\n",
    "loss = nn.MSELoss()\n",
    "# calling optimizer: SGD\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dca03545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1: w = 0.30486929416656494 ,l=21.84905815\n",
      "epoch: 11: w = 1.3814506530761719 ,l=0.80809522\n",
      "epoch: 21: w = 1.5648349523544312 ,l=0.24958242\n",
      "epoch: 31: w = 1.6042473316192627 ,l=0.22182210\n",
      "epoch: 41: w = 1.6202094554901123 ,l=0.20856853\n",
      "epoch: 51: w = 1.632115125656128 ,l=0.19641997\n",
      "epoch: 61: w = 1.6430925130844116 ,l=0.18498704\n",
      "epoch: 71: w = 1.6536530256271362 ,l=0.17421982\n",
      "epoch: 81: w = 1.663886547088623 ,l=0.16407934\n",
      "epoch: 91: w = 1.6738152503967285 ,l=0.15452912\n",
      "Prediction after training:f(5) =  9.346\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(y,y_pred)\n",
    "    \n",
    "    # backward prop: dl/dw\n",
    "    l.backward()\n",
    "    # update weights:\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # zero gradient: w.grad = 0\n",
    "    optimizer.zero_grad() \n",
    "    if epoch%10==0:\n",
    "        # unpack weight and bias\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch: {epoch+1}: w = {w[0][0]} ,l={l:.8f}')\n",
    "        \n",
    "\n",
    "# Final prediction\n",
    "print(f'Prediction after training:f(5) =  {model(x_test).item():.3f}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9288cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing same using our custom Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3e00f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "model  = LinearRegression(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4514126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE ran the above custom function while commenting previously defined model.\n",
    "# we got almost the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbb1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a8185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
